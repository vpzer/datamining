{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrucciones\n",
    "\n",
    "El laboratorio tiene 6 ptos, donde obtener 6 ptos equivale a un 7.0 y 0 ptos un 1.0. \n",
    "\n",
    "El formato de entrega será subir a u-cursos un Jupyter notebook\n",
    "laboratorio7.ipynb, que se debe ejecutar sin errores desde la primera celda a la última. Todo el código debe estar en el mismo notebook, el código debe estar comentado y testeado, el notebook debe estar escrito en forma de informe técnico, escribiendo una celda markdown antes o después de cada celda de código que arroja algún output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Laboratorio 7: Text Mining\n",
    "\n",
    "\n",
    "## 1. Investigación, 2.5ptos (0.5ptos c/u)\n",
    "\n",
    "1. ¿Qué es el modelado de tópicos?¿Para qué sirve? De dos ejemplos de uso.\n",
    "2. Explique LDA, detalle que significa cada unos de los parámetros ($\\theta_{d}, \\beta_{k}, \\alpha, \\eta$).\n",
    "3. Mencione algún uso alternativo de LDA a parte de encontrar tópicos.\n",
    "4. Investigue otro modelo de tópicos y expliquelo, comenté en que se diferencia de LDA.\n",
    "5. Investigue y explique al menos 2 técnicas para determinar el número óptimo de tópicos. ¿Cuál es el problema de usar métricas para escoger el número óptimo de tópicos? \n",
    "\n",
    "## 2. Aplicación, 3.5 ptos\n",
    "\n",
    "2.1 (0.5 ptos) Cree una función para procesar los textos e imprima en pantalla un ejemplo del dataset sin y con procesar. \n",
    "Justifique los supuestos realizados. ¿A cuánto se reduce el vocabulario? \n",
    "\n",
    "2.2. (0.5 ptos) Postprocessing: análisis de palabras muy frecuentes, poco frecuentes, errores ortográficos, etc. Justifique los supuestos realizados para reducir el vocabulario aún más.\n",
    "\n",
    "2.3. (2.5 ptos) LDA \n",
    "\n",
    "2.3.1 (1.0 ptos) Ejecute LDA para k=2, ..., 20 y escoga el número óptimo de tópicos en base a las métricas investigadas y su juicio.\n",
    "\n",
    "2.3.2 (1.0 ptos) Interprete y etiquete los tópicos encontrados. \n",
    "Hint: para esto analice las palabras más relevantes de cada uno de los tópicos y los documentos donde los tópicos pesan más. \n",
    "\n",
    "2.3.3 (0.5 ptos) Analice el tamaño de los tópicos y concluya. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de datos\n",
    "\n",
    "El dataset del laboratorio es el 20 newsgroups, comprime alrededor de 18.000 noticios en 20 tópicos (ya vienen etiquetadas de antemano). La partición train (60%) y test (40%) viene por defecto.\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimina los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "#Preprocesamiento\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS #importar set de stopwords\n",
    "from nltk.stem import SnowballStemmer #importar stemmer\n",
    "nlp = spacy.load('en_core_web_sm') #python -m spacy download en_core_web_sm\n",
    "\n",
    "#Bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "#Topic modeling\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "#Importar dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
