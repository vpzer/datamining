{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrucciones\n",
    "\n",
    "El laboratorio tiene 6 ptos, donde obtener 6 ptos equivale a un 7.0 y 0 ptos un 1.0. \n",
    "\n",
    "El formato de entrega será subir a u-cursos un Jupyter notebook\n",
    "laboratorio7.ipynb, que se debe ejecutar sin errores desde la primera celda a la última. Todo el código debe estar en el mismo notebook, el código debe estar comentado y testeado, el notebook debe estar escrito en forma de informe técnico, escribiendo una celda markdown antes o después de cada celda de código que arroja algún output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Laboratorio 7: Text Mining\n",
    "\n",
    "\n",
    "## 1. Investigación, 2.5ptos (0.5ptos c/u)\n",
    "\n",
    "1. ¿Qué es el modelado de tópicos?¿Para qué sirve? De dos ejemplos de uso.\n",
    "2. Explique LDA, detalle que significa cada unos de los parámetros ($\\theta_{d}, \\beta_{k}, \\alpha, \\eta$).\n",
    "3. Mencione algún uso alternativo de LDA a parte de encontrar tópicos.\n",
    "4. Investigue otro modelo de tópicos y expliquelo, comenté en que se diferencia de LDA.\n",
    "5. Investigue y explique al menos 2 técnicas para determinar el número óptimo de tópicos. ¿Cuál es el problema de usar métricas para escoger el número óptimo de tópicos? \n",
    "\n",
    "## 2. Aplicación, 3.5 ptos\n",
    "\n",
    "2.1 (0.5 ptos) Cree una función para procesar los textos e imprima en pantalla un ejemplo del dataset sin y con procesar. \n",
    "Justifique los supuestos realizados. ¿A cuánto se reduce el vocabulario? \n",
    "\n",
    "2.2. (0.5 ptos) Postprocessing: análisis de palabras muy frecuentes, poco frecuentes, errores ortográficos, etc. Justifique los supuestos realizados para reducir el vocabulario aún más.\n",
    "\n",
    "2.3. (2.5 ptos) LDA \n",
    "\n",
    "2.3.1 (1.0 ptos) Ejecute LDA para k=2, ..., 20 y escoga el número óptimo de tópicos en base a las métricas investigadas y su juicio.\n",
    "\n",
    "2.3.2 (1.0 ptos) Interprete y etiquete los tópicos encontrados. \n",
    "Hint: para esto analice las palabras más relevantes de cada uno de los tópicos y los documentos donde los tópicos pesan más. \n",
    "\n",
    "2.3.3 (0.5 ptos) Analice el tamaño de los tópicos y concluya. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de datos\n",
    "\n",
    "El dataset del laboratorio es el 20 newsgroups, comprime alrededor de 18.000 noticios en 20 tópicos (ya vienen etiquetadas de antemano). La partición train (60%) y test (40%) viene por defecto.\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimina los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "#Preprocesamiento\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS #importar set de stopwords\n",
    "from nltk.stem import SnowballStemmer #importar stemmer\n",
    "nlp = spacy.load('en_core_web_sm') #python -m spacy download en_core_web_sm\n",
    "\n",
    "#Bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "#Topic modeling\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc, sep=None, stopwords_remove =False, lemmatization=False, stemming = False, vocabulary=None):\n",
    "    '''\n",
    "    Por defecto divide la sentencia por el carácter espacio.\n",
    "    Ej: 'Data Mining is the best course'->['Data',  'Mining', 'is', 'the', 'best', 'course']\n",
    "    \n",
    "    Input: \n",
    "    1. doc: str, documento.\n",
    "    2. sep: str, carácter para dividir el documento en tokens, por defecto es el espacio.\n",
    "    3. stopwords_remove: bool, si es True remueve los stopwords del documento.\n",
    "    4. lemmatization: bool, si es True lleva las palabras a su lema.\n",
    "    5. stemming: bool, si es True lleva las palabas a su raíz.\n",
    "    6. vocabuary: list, si un vocabulario es dado filtra las palabras que no estan presentes en el.\n",
    "    \n",
    "    Output: \n",
    "    list, lista de tokens.\n",
    "    '''\n",
    "    doc = re.sub(r'\\S+@\\S+', '', doc) #elimina correos electrónicos \n",
    "    doc = re.sub(r'[^\\w\\s]','', doc) #elimina los símbolos de puntuación excepto underscore\n",
    "    doc = re.sub(r'[a-zA-Z]+[0-9]+', '', doc) #elimina los caracteres que contienen letras y números\n",
    "    doc = re.sub(r'[0-9]+', ' ', doc) #elimina los caracteres numéricos\n",
    "   \n",
    "    tokens = doc.split(sep) #tokenización\n",
    "    tokens = [word.lower() for word in tokens] #pasar todas las palabras a minúsculas\n",
    "    \n",
    "    if stopwords_remove ==True: #remover stopwords y palabras con menos de cuatro caracteres\n",
    "        tokens = [word for word in tokens if word not in STOP_WORDS and len(word)>3]\n",
    "    \n",
    "    if lemmatization==True:\n",
    "        tokens = [nlp(word)[0].lemma_ for word in tokens]\n",
    "        \n",
    "    if stemming == True:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if vocabulary is not None:\n",
    "        tokens = [word for word in tokens if word in vocabulary]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "#Obtener bag-of-words del corpus\n",
    "args = {\"stopwords_remove\": True, 'stemming': True}\n",
    "tf_vectorizer = CountVectorizer(analyzer='word', tokenizer=lambda text: tokenizer(text, **args), max_df = 0.8, min_df = 5)\n",
    "#BOW del corpus\n",
    "%time bow = tf_vectorizer.fit_transform(newsgroups_train['data'])\n",
    "#Extrae vocabulario\n",
    "vocabulary = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus Train en formato LDA\n",
    "train_tokenized = [tokenizer(doc, stopwords_remove=True, stemming = True, vocabulary=vocabulary) for doc in newsgroups_train['data']]\n",
    "dictionary = Dictionary(train_tokenized)\n",
    "corpus_train = [dictionary.doc2bow(text) for text in train_tokenized]\n",
    "#Corpus Test en formato LDA\n",
    "test_tokenized = [tokenizer(doc, stopwords_remove=True, stemming = True, vocabulary=vocabulary) for doc in newsgroups_test['data']]\n",
    "corpus_test = [dictionary.doc2bow(text) for text in test_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el diccionario y el corpus\n",
    "dictionary.save('dictionary.dict')\n",
    "MmCorpus.serialize('corpus.mm', corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tamaño del vocabulario', len(vocabulary))\n",
    "print('Primer elemento del diccionario o bolsa de palabras:', dictionary[0])\n",
    "print('Representación del corpus en el formato que requiere la librería: \\n', corpus_train[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrenamos lda en el corpus\n",
    "lda_model = LdaModel(corpus=corpus_train, id2word=dictionary, num_topics=20, alpha='auto', eta='auto', random_state=0) \n",
    "#guardamos el modelo\n",
    "lda_model.save('lda_model.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printear las topn palabras más probables de un tópico\n",
    "lda_model.show_topic(topicid=1, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mezcla de tópicos para un documento\n",
    "lda_model.get_document_topics(corpus_train[100], minimum_probability=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus_train, dictionary=dictionary, sort_topics=True, R=30)\n",
    "pyLDAvis.display(lda_vis, template_type='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_values(dictionary, corpus_train, corpus_test, limit, start=2, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus_train : Gensim corpus de entrenamiento\n",
    "    corpus_test: Gensim corpus de test\n",
    "    limit : int, máximo número de topicos\n",
    "    start : int, mínimo número de tópicos\n",
    "    sep: int, tamaño del salto entre número de tópicos\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : Lista de LDA topic models\n",
    "    perplexity_values : Lista de las perplexties de los LDA topic models\n",
    "    \"\"\"\n",
    "    perplexity_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit+1, step):\n",
    "        model = LdaModel(corpus=corpus_train, num_topics=num_topics, id2word=dictionary, alpha=10/num_topics, eta=100 , random_state=0)\n",
    "        model_list.append(model)\n",
    "        perplexity = 2**(-model.log_perplexity(corpus_test)) \n",
    "        perplexity_values.append(perplexity)\n",
    "\n",
    "    return model_list, perplexity_values\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus_train, texts, limit, start=2, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus_train : Gensim corpus de entrenamiento\n",
    "    texts : Lista de textos de entrada\n",
    "    limit : int, máximo número de topicos\n",
    "    start : int, mínimo número de tópicos\n",
    "    sep: int, tamaño del salto entre número de tópicos\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : Lista de LDA topic models\n",
    "    coherence_values : Lista de las coherence values de los LDA topic models\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit+1, step):\n",
    "        model = LdaModel(corpus=corpus_train, num_topics=num_topics, id2word=dictionary, alpha='auto', eta='auto', random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, np.array(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_list, perplexity_values = compute_perplexity_values(dictionary=dictionary, corpus_train=corpus_train, corpus_test=corpus_test, start=2, limit=20, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus_train, texts=data_tokenized, start=2, limit=20, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2, 21, 1)\n",
    "plt.plot(x, perplexity_values, '-o')\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.legend((\"perplexity_values\"), loc='best')\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2, 21, 1)\n",
    "plt.plot(x, coherence_values,'-o')\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
